{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 80. ID番号への変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "categories = ['b', 't', 'e', 'm']\n",
    "category_names = ['business', 'science and technology', 'entertainment', 'health']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x):\n",
    "    x = re.sub(r'\\s+', ' ', x)\n",
    "    x = nlp.make_doc(x)\n",
    "    x = [d.text for d in x]\n",
    "    return x\n",
    "\n",
    "def read_feature_dataset(filename):\n",
    "    with open(filename) as f:\n",
    "        dataset = f.read().splitlines()\n",
    "    dataset = [line.split('\\t') for line in dataset]\n",
    "    dataset_t = [categories.index(line[0]) for line in dataset]\n",
    "    dataset_x = [tokenize(line[1]) for line in dataset]\n",
    "    return dataset_x, torch.tensor(dataset_t, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_t = read_feature_dataset('data/train.txt')\n",
    "valid_x, valid_t = read_feature_dataset('data/valid.txt')\n",
    "test_x, test_t = read_feature_dataset('data/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9700"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = Counter([\n",
    "    x\n",
    "    for sent in train_x\n",
    "    for x in sent])\n",
    "\n",
    "vocab_in_train = [\n",
    "    token\n",
    "    for token, freq in counter.most_common()\n",
    "    if freq > 1]\n",
    "len(vocab_in_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "語彙を用意し，ID番号の列に変換できるようにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = ['[UNK]'] + vocab_in_train\n",
    "vocab_dict = {x:n for n, x in enumerate(vocab_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_ids(sent):\n",
    "    return torch.tensor([vocab_dict[x if x in vocab_dict else '[UNK]'] for x in sent], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kathleen', 'Sebelius', \"'\", 'LGBT', 'legacy']\n",
      "tensor([   0,    0,    2, 2648,    0])\n"
     ]
    }
   ],
   "source": [
    "print(train_x[0])\n",
    "print(sent_to_ids(train_x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ID番号の列に変換しておく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_ids(dataset):\n",
    "    return [sent_to_ids(x) for x in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_s = dataset_to_ids(train_x)\n",
    "valid_s = dataset_to_ids(valid_x)\n",
    "test_s = dataset_to_ids(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([   0,    0,    2, 2648,    0]),\n",
       " tensor([   9, 6740, 1445, 2076,  583,   10,  547,   32,   51,  873, 6741]),\n",
       " tensor([   0,  205, 4198,  315, 1899, 1232,    0])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_s[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 81. RNNによる予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence as pad\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
    "from torch.nn.utils.rnn import pad_packed_sequence as unpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    def __init__(self, source, target = None, lengths = None):\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "        self.lengths = lengths\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.source.shape[1]\n",
    "    \n",
    "    def send(self, device):\n",
    "        self.source = self.source.to(device)\n",
    "        if self.target is not None:\n",
    "            self.target = self.target.to(device)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, source, target):\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "        self.lengths = torch.tensor([len(x) for x in source])\n",
    "        self.size = len(source)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "            'src':self.source[index],\n",
    "            'trg':self.target[index],\n",
    "            'lengths':self.lengths[index]}\n",
    "    \n",
    "    def collate(self, xs):\n",
    "        src = pad([x['src'] for x in xs])\n",
    "        trg = torch.stack([x['trg'] for x in xs], dim=-1)\n",
    "        lengs = torch.stack([x['lengths'] for x in xs], dim=-1)\n",
    "        return Batch(src, trg, lengs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler(torch.utils.data.Sampler):\n",
    "    def __init__(self, dataset, width, shuffle = False):\n",
    "        self.dataset = dataset\n",
    "        self.width = width\n",
    "        self.shuffle = shuffle\n",
    "        self.batches = None\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.batches is None:\n",
    "            self.batches = self.generate_batches()\n",
    "        return len(self.batches)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        if self.batches is None:\n",
    "            self.batches = self.generate_batches()\n",
    "        for batch in self.batches:\n",
    "            yield batch\n",
    "        self.batches = None\n",
    "        \n",
    "    def generate_indices(self):\n",
    "        if self.shuffle:\n",
    "            self.indices = torhc.randperm(len(self.dataset))\n",
    "        if not hasattr(self, 'indices'):\n",
    "            self.indices = torch.arange(len(self.dataset))\n",
    "        return self.indices\n",
    "        \n",
    "    def generate_batches(self):\n",
    "        index = 0\n",
    "        indices = self.generate_indices()\n",
    "        batches = []\n",
    "        while index < len(self.dataset):\n",
    "            batch = indices[index : index + self.width]\n",
    "            index += self.width\n",
    "            batches.append(batch)\n",
    "        if self.shuffle:\n",
    "            rd.shuffle(batches)\n",
    "        return batches\n",
    "\n",
    "class DescendingSampler(Sampler):\n",
    "    def generate_indices(self):\n",
    "        if not hasattr(self, 'indices'):\n",
    "            self.indices = torch.arange(len(self.dataset))\n",
    "            self.indices = self.indices[self.dataset.lengths[self.indices].argsort(descending=True)]\n",
    "        return self.indices\n",
    "        \n",
    "class MaxTokensSampler(DescendingSampler):\n",
    "    def generate_batches(self):\n",
    "        batches = []\n",
    "        batch = []\n",
    "        acc = 0\n",
    "        max_len = 0\n",
    "        indices = self.generate_indices()\n",
    "        for index in self.indices:\n",
    "            acc += 1\n",
    "            this_len = self.dataset.lengths[index]\n",
    "            max_len = max(max_len, this_len)\n",
    "            if acc * max_len > self.width:\n",
    "                batches.append(batch)\n",
    "                batch = [index]\n",
    "                acc = 1\n",
    "                max_len = this_len\n",
    "            else:\n",
    "                batch.append(index)\n",
    "        if batch != []:\n",
    "            batches.append(batch)\n",
    "        if self.shuffle:\n",
    "            rd.shuffle(batches)\n",
    "        return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_loader(dataset, width, sampler=Sampler, shuffle=False, num_workers=0):\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset, \n",
    "        batch_sampler = sampler(dataset, width, shuffle),\n",
    "        collate_fn = dataset.collate,\n",
    "        num_workers = num_workers)\n",
    "\n",
    "def gen_descending_loader(dataset, width, num_workers=0):\n",
    "    return gen_loader(dataset, width, sampler = DescendingSampler, shuffle = False, num_workers = num_workers)\n",
    "\n",
    "def gen_maxtokens_loader(dataset, width, num_workers=0):\n",
    "    return gen_loader(dataset, width, sampler = MaxTokensSampler, shuffle = True, num_workers = num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットを用意する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train_s, train_t)\n",
    "valid_dataset = Dataset(valid_s, valid_t)\n",
    "test_dataset = Dataset(test_s, test_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMのモデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, v_size, e_size, h_size, c_size, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(v_size, e_size)\n",
    "        self.rnn = nn.LSTM(e_size, h_size, num_layers = 1)\n",
    "        self.out = nn.Linear(h_size, c_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, batch, h=None):\n",
    "        x = self.embed(batch.source)\n",
    "        x = pack(x, batch.lengths)\n",
    "        x, (h, c) = self.rnn(x, h)\n",
    "        h = self.out(h)\n",
    "        return h.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier(len(vocab_dict), 300, 50, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "予測する(問題文にあるsoftmaxはかけてない)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 3, 0, 0, 3, 3, 0, 0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = gen_loader(test_dataset, 10, DescendingSampler, False)\n",
    "model(iter(loader).next()).argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 82. 確率的勾配降下法による学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TaskとTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task:\n",
    "    def __init__(self):\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def train_step(self, model, batch):\n",
    "        model.zero_grad()\n",
    "        loss = self.criterion(model(batch), batch.target)\n",
    "        loss.backward()\n",
    "        return loss.item()\n",
    "    \n",
    "    def valid_step(self, model, batch):\n",
    "        with torch.no_grad():\n",
    "            loss = self.criterion(model(batch), batch.target)\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, loaders, task, optimizer, max_iter, device):\n",
    "        self.model = model\n",
    "        self.model.to(device)\n",
    "        self.train_loader, self.valid_loader = loaders\n",
    "        self.task = task\n",
    "        self.optimizer = optimizer\n",
    "        self.max_iter = max_iter\n",
    "        self.device = device\n",
    "        \n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        accum = 0.0\n",
    "        examples = 0\n",
    "        for batch in self.train_loader:\n",
    "            batch.send(self.device)\n",
    "            accum += self.task.train_step(self.model, batch) * len(batch)\n",
    "            examples += len(batch)\n",
    "            self.optimizer.step()\n",
    "        return accum / examples\n",
    "            \n",
    "    def valid_epoch(self):\n",
    "        self.model.eval()\n",
    "        accum = 0.0\n",
    "        examples = 0\n",
    "        for batch in self.valid_loader:\n",
    "            batch.send(self.device)\n",
    "            accum += self.task.valid_step(self.model, batch) * len(batch)\n",
    "            examples += len(batch)\n",
    "        return accum / examples\n",
    "    \n",
    "    def train(self):\n",
    "        for epoch in range(self.max_iter):\n",
    "            train_loss = self.train_epoch()\n",
    "            valid_loss = self.valid_epoch()\n",
    "            line = 'epoch {}, train_loss:{:.5f}, valid_loss:{:.5f}'.format(epoch, train_loss, valid_loss)\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train_loss:0.96342, valid_loss:1.29103\n",
      "epoch 1, train_loss:0.66394, valid_loss:1.21547\n",
      "epoch 2, train_loss:0.48932, valid_loss:1.33933\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "model = LSTMClassifier(len(vocab_dict), 300, 128, 4)\n",
    "loaders = (\n",
    "    gen_loader(train_dataset, 1),\n",
    "    gen_loader(valid_dataset, 1))\n",
    "task = Task()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True)\n",
    "trainer = Trainer(model, loaders, task, optimizer, 3, device)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "予測もする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    def __init__(self, model, loader, device):\n",
    "        self.model = model\n",
    "        self.loader = loader\n",
    "        self.device = device\n",
    "        \n",
    "    def infer(self, batch):\n",
    "        self.model.eval()\n",
    "        batch.send(self.device)\n",
    "        return self.model(batch).argmax(dim=-1).item()\n",
    "        \n",
    "    def predict(self):\n",
    "        lst = []\n",
    "        for batch in self.loader:\n",
    "            lst.append(self.infer(batch))\n",
    "        return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(true, pred):\n",
    "    return np.mean([t == p for t, p in zip(true, pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習データでの正解率 : 0.8289966304754773\n"
     ]
    }
   ],
   "source": [
    "predictor = Predictor(model, gen_loader(train_dataset, 1), device)\n",
    "pred = predictor.predict()\n",
    "print('学習データでの正解率 :', accuracy(train_t, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "評価データでの正解率 : 0.6594311377245509\n"
     ]
    }
   ],
   "source": [
    "predictor = Predictor(model, gen_loader(test_dataset, 1), device)\n",
    "pred = predictor.predict()\n",
    "print('評価データでの正解率 :', accuracy(test_t, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 83. ミニバッチ化・GPU上での学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train_loss:1.08424, valid_loss:0.99817\n",
      "epoch 1, train_loss:0.80309, valid_loss:0.82198\n",
      "epoch 2, train_loss:0.56469, valid_loss:0.79174\n",
      "epoch 3, train_loss:0.39283, valid_loss:0.69517\n",
      "epoch 4, train_loss:0.25630, valid_loss:0.80450\n",
      "epoch 5, train_loss:0.15934, valid_loss:0.71693\n",
      "epoch 6, train_loss:0.09452, valid_loss:0.71515\n",
      "epoch 7, train_loss:0.04743, valid_loss:0.77099\n",
      "epoch 8, train_loss:0.02807, valid_loss:0.80126\n",
      "epoch 9, train_loss:0.01790, valid_loss:0.82775\n"
     ]
    }
   ],
   "source": [
    "model = LSTMClassifier(len(vocab_dict), 300, 128, 4)\n",
    "loaders = (\n",
    "    gen_maxtokens_loader(train_dataset, 4000),\n",
    "    gen_descending_loader(valid_dataset, 128))\n",
    "task = Task()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.2, momentum=0.9, nesterov=True)\n",
    "trainer = Trainer(model, loaders, task, optimizer, 10, device)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習データでの正解率 : 0.9985024335454886\n"
     ]
    }
   ],
   "source": [
    "predictor = Predictor(model, gen_loader(train_dataset, 1), device)\n",
    "pred = predictor.predict()\n",
    "print('学習データでの正解率 :', accuracy(train_t, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "評価データでの正解率 : 0.7941616766467066\n"
     ]
    }
   ],
   "source": [
    "predictor = Predictor(model, gen_loader(test_dataset, 1), device)\n",
    "pred = predictor.predict()\n",
    "print('評価データでの正解率 :', accuracy(test_t, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 84. 単語ベクトルの導入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koyama.s/nlp10021/envs/nlp10021/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_embed(embed):\n",
    "    for i, token in enumerate(vocab_list):\n",
    "        if token in vectors:\n",
    "            embed.weight.data[i] = torch.from_numpy(vectors[token])\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-35-064ca4b82e24>:4: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
      "  embed.weight.data[i] = torch.from_numpy(vectors[token])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train_loss:1.12420, valid_loss:1.04449\n",
      "epoch 1, train_loss:0.94495, valid_loss:0.86126\n",
      "epoch 2, train_loss:0.66232, valid_loss:0.63730\n",
      "epoch 3, train_loss:0.54309, valid_loss:0.58128\n",
      "epoch 4, train_loss:0.50897, valid_loss:0.56129\n",
      "epoch 5, train_loss:0.46464, valid_loss:0.52984\n",
      "epoch 6, train_loss:0.46994, valid_loss:0.50143\n",
      "epoch 7, train_loss:0.40089, valid_loss:0.48184\n",
      "epoch 8, train_loss:0.37006, valid_loss:0.44809\n",
      "epoch 9, train_loss:0.33311, valid_loss:0.43202\n"
     ]
    }
   ],
   "source": [
    "model = LSTMClassifier(len(vocab_dict), 300, 128, 4)\n",
    "init_embed(model.embed)\n",
    "task = Task()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.9, nesterov=True)\n",
    "trainer = Trainer(model, loaders, task, optimizer, 10, device)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習データでの正解率 : 0.8927368026956196\n"
     ]
    }
   ],
   "source": [
    "predictor = Predictor(model, gen_loader(train_dataset, 1), device)\n",
    "pred = predictor.predict()\n",
    "print('学習データでの正解率 :', accuracy(train_t, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "評価データでの正解率 : 0.8622754491017964\n"
     ]
    }
   ],
   "source": [
    "predictor = Predictor(model, gen_loader(test_dataset, 1), device)\n",
    "pred = predictor.predict()\n",
    "print('評価データでの正解率 :', accuracy(test_t, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 85. 双方向RNN・多層化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, v_size, e_size, h_size, c_size, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(v_size, e_size)\n",
    "        self.rnn = nn.LSTM(e_size, h_size, num_layers = 2, bidirectional = True)\n",
    "        self.out = nn.Linear(h_size * 2, c_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, batch, h=None):\n",
    "        x = self.embed(batch.source)\n",
    "        x = pack(x, batch.lengths)\n",
    "        x, (h, c) = self.rnn(x, h)\n",
    "        h = h[-2:]\n",
    "        h = h.transpose(0,1)\n",
    "        h = h.contiguous().view(-1, h.size(1) * h.size(2))\n",
    "        h = self.out(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train_loss:1.18692, valid_loss:1.12214\n",
      "epoch 1, train_loss:0.98376, valid_loss:0.87834\n",
      "epoch 2, train_loss:0.70407, valid_loss:0.66558\n",
      "epoch 3, train_loss:0.56156, valid_loss:0.61257\n",
      "epoch 4, train_loss:0.51531, valid_loss:0.59589\n",
      "epoch 5, train_loss:0.49292, valid_loss:0.55508\n",
      "epoch 6, train_loss:0.46636, valid_loss:0.53921\n",
      "epoch 7, train_loss:0.42618, valid_loss:0.53172\n",
      "epoch 8, train_loss:0.40390, valid_loss:0.49850\n",
      "epoch 9, train_loss:0.35865, valid_loss:0.44620\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTMClassifier(len(vocab_dict), 300, 128, 4)\n",
    "init_embed(model.embed)\n",
    "task = Task()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.9, nesterov=True)\n",
    "trainer = Trainer(model, loaders, task, optimizer, 10, device)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習データでの正解率 : 0.8699925121677274\n"
     ]
    }
   ],
   "source": [
    "predictor = Predictor(model, gen_loader(train_dataset, 1), device)\n",
    "pred = predictor.predict()\n",
    "print('学習データでの正解率 :', accuracy(train_t, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "評価データでの正解率 : 0.8360778443113772\n"
     ]
    }
   ],
   "source": [
    "predictor = Predictor(model, gen_loader(test_dataset, 1), device)\n",
    "pred = predictor.predict()\n",
    "print('評価データでの正解率 :', accuracy(test_t, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 86. 畳み込みニューラルネットワーク (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PADのあるデータセットにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_vocab_list = ['[PAD]', '[UNK]'] + vocab_in_train\n",
    "cnn_vocab_dict = {x:n for n, x in enumerate(cnn_vocab_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kathleen', 'Sebelius', \"'\", 'LGBT', 'legacy']\n",
      "tensor([   1,    1,    3, 2649,    1])\n"
     ]
    }
   ],
   "source": [
    "def cnn_sent_to_ids(sent):\n",
    "    return torch.tensor([cnn_vocab_dict[x if x in cnn_vocab_dict else '[UNK]'] for x in sent], dtype=torch.long)\n",
    "\n",
    "print(train_x[0])\n",
    "print(cnn_sent_to_ids(train_x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_dataset_to_ids(dataset):\n",
    "    return [cnn_sent_to_ids(x) for x in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_train_s = cnn_dataset_to_ids(train_x)\n",
    "cnn_valid_s = cnn_dataset_to_ids(valid_x)\n",
    "cnn_test_s = cnn_dataset_to_ids(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([   1,    1,    3, 2649,    1]),\n",
       " tensor([  10, 6741, 1446, 2077,  584,   11,  548,   33,   52,  874, 6742]),\n",
       " tensor([   1,  206, 4199,  316, 1900, 1233,    1])]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_train_s[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBatch:\n",
    "    def __init__(self, source, target = None, mask = None):\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "        self.mask = mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.source.shape[1]\n",
    "    \n",
    "    def send(self, device):\n",
    "        self.source = self.source.to(device)\n",
    "        if self.target is not None:\n",
    "            self.target = self.target.to(device)\n",
    "        if self.mask is not None:\n",
    "            self.mask = self.mask.to(device)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNDataset(Dataset):\n",
    "    def collate(self, xs):\n",
    "        max_seq_len = max([x['lengths'] for x in xs])\n",
    "        src = [torch.cat([x['src'], torch.zeros(max_seq_len - x['lengths'], dtype=torch.long)], dim=-1) for x in xs]\n",
    "        src = torch.stack(src)\n",
    "        mask = [[1] * x['lengths'] + [0] * (max_seq_len - x['lengths']) for x in xs]\n",
    "        mask = torch.tensor(mask, dtype=torch.long)\n",
    "        trg = torch.tensor([x['trg'] for x in xs])\n",
    "        return CNNBatch(src, trg, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_train_dataset = CNNDataset(cnn_train_s, train_t)\n",
    "cnn_valid_dataset = CNNDataset(cnn_valid_s, valid_t)\n",
    "cnn_test_dataset = CNNDataset(cnn_test_s, test_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNNモデルをつくっていく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, v_size, e_size, h_size, c_size, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(v_size, e_size)\n",
    "        self.conv = nn.Conv1d(e_size, h_size, 3, padding=1)\n",
    "        self.act = nn.ReLU()\n",
    "        self.out = nn.Linear(h_size, c_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        x = self.embed(batch.source)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv(x.transpose(-1, -2))\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x.masked_fill_(batch.mask.unsqueeze(-2) == 0, -1e4)\n",
    "        x = F.max_pool1d(x, x.size(-1)).squeeze(-1)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 87. 確率的勾配降下法によるCNNの学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習させる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_cnn_embed(embed):\n",
    "    for i, token in enumerate(cnn_vocab_list):\n",
    "        if token in vectors:\n",
    "            embed.weight.data[i] = torch.from_numpy(vectors[token])\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train_loss:1.02734, valid_loss:0.98663\n",
      "epoch 1, train_loss:0.86125, valid_loss:0.89180\n",
      "epoch 2, train_loss:0.76665, valid_loss:0.81268\n",
      "epoch 3, train_loss:0.67993, valid_loss:0.73744\n",
      "epoch 4, train_loss:0.61030, valid_loss:0.68223\n",
      "epoch 5, train_loss:0.56465, valid_loss:0.64216\n",
      "epoch 6, train_loss:0.52255, valid_loss:0.62114\n",
      "epoch 7, train_loss:0.49367, valid_loss:0.61635\n",
      "epoch 8, train_loss:0.47122, valid_loss:0.58231\n",
      "epoch 9, train_loss:0.44774, valid_loss:0.55935\n"
     ]
    }
   ],
   "source": [
    "model = CNNClassifier(len(cnn_vocab_dict), 300, 128, 4)\n",
    "init_cnn_embed(model.embed)\n",
    "loaders = (\n",
    "    gen_maxtokens_loader(cnn_train_dataset, 4000),\n",
    "    gen_descending_loader(cnn_valid_dataset, 32))\n",
    "task = Task()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True)\n",
    "trainer = Trainer(model, loaders, task, optimizer, 10, device)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習データでの正解率 : 0.8410707600149757\n"
     ]
    }
   ],
   "source": [
    "predictor = Predictor(model, gen_loader(cnn_train_dataset, 1), device)\n",
    "pred = predictor.predict()\n",
    "print('学習データでの正解率 :', accuracy(train_t, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "評価データでの正解率 : 0.8211077844311377\n"
     ]
    }
   ],
   "source": [
    "predictor = Predictor(model, gen_loader(cnn_test_dataset, 1), device)\n",
    "pred = predictor.predict()\n",
    "print('評価データでの正解率 :', accuracy(test_t, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 88. パラメータチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train_loss:1.01739, valid_loss:0.96067\n",
      "epoch 1, train_loss:0.78486, valid_loss:0.77479\n",
      "epoch 2, train_loss:0.61598, valid_loss:0.65609\n",
      "epoch 3, train_loss:0.53782, valid_loss:0.60637\n",
      "epoch 4, train_loss:0.48809, valid_loss:0.56731\n",
      "epoch 5, train_loss:0.44536, valid_loss:0.55024\n",
      "epoch 6, train_loss:0.41046, valid_loss:0.52335\n",
      "epoch 7, train_loss:0.38094, valid_loss:0.49093\n",
      "epoch 8, train_loss:0.35429, valid_loss:0.45857\n",
      "epoch 9, train_loss:0.33033, valid_loss:0.43609\n",
      "評価データでの正解率 : 0.8675149700598802\n",
      "epoch 0, train_loss:1.01111, valid_loss:0.92879\n",
      "epoch 1, train_loss:0.77630, valid_loss:0.78188\n",
      "epoch 2, train_loss:0.62220, valid_loss:0.66524\n",
      "epoch 3, train_loss:0.54864, valid_loss:0.61105\n",
      "epoch 4, train_loss:0.48616, valid_loss:0.58252\n",
      "epoch 5, train_loss:0.44996, valid_loss:0.53688\n",
      "epoch 6, train_loss:0.40628, valid_loss:0.50916\n",
      "epoch 7, train_loss:0.37484, valid_loss:0.48390\n",
      "epoch 8, train_loss:0.34760, valid_loss:0.46340\n",
      "epoch 9, train_loss:0.32076, valid_loss:0.45185\n",
      "評価データでの正解率 : 0.8585329341317365\n",
      "epoch 0, train_loss:1.00008, valid_loss:0.94392\n",
      "epoch 1, train_loss:0.76109, valid_loss:0.77081\n",
      "epoch 2, train_loss:0.62565, valid_loss:0.66987\n",
      "epoch 3, train_loss:0.54519, valid_loss:0.61905\n",
      "epoch 4, train_loss:0.48980, valid_loss:0.57843\n",
      "epoch 5, train_loss:0.45460, valid_loss:0.56182\n",
      "epoch 6, train_loss:0.41623, valid_loss:0.52299\n",
      "epoch 7, train_loss:0.38564, valid_loss:0.49871\n",
      "epoch 8, train_loss:0.35247, valid_loss:0.46236\n",
      "epoch 9, train_loss:0.32978, valid_loss:0.45055\n",
      "評価データでの正解率 : 0.8562874251497006\n",
      "epoch 0, train_loss:1.01035, valid_loss:0.93371\n",
      "epoch 1, train_loss:0.77598, valid_loss:0.77075\n",
      "epoch 2, train_loss:0.62909, valid_loss:0.66898\n",
      "epoch 3, train_loss:0.54485, valid_loss:0.61625\n",
      "epoch 4, train_loss:0.49427, valid_loss:0.57700\n",
      "epoch 5, train_loss:0.44175, valid_loss:0.54695\n",
      "epoch 6, train_loss:0.40172, valid_loss:0.54102\n",
      "epoch 7, train_loss:0.37186, valid_loss:0.49365\n",
      "epoch 8, train_loss:0.33627, valid_loss:0.48425\n",
      "epoch 9, train_loss:0.31433, valid_loss:0.46774\n",
      "評価データでの正解率 : 0.8562874251497006\n",
      "epoch 0, train_loss:1.10917, valid_loss:0.90587\n",
      "epoch 1, train_loss:0.75955, valid_loss:0.74050\n",
      "epoch 2, train_loss:0.63175, valid_loss:0.66844\n",
      "epoch 3, train_loss:0.55358, valid_loss:0.61308\n",
      "epoch 4, train_loss:0.48281, valid_loss:0.57076\n",
      "epoch 5, train_loss:0.44514, valid_loss:0.56686\n",
      "epoch 6, train_loss:0.39718, valid_loss:0.51880\n",
      "epoch 7, train_loss:0.37114, valid_loss:0.51398\n",
      "epoch 8, train_loss:0.33704, valid_loss:0.47312\n",
      "epoch 9, train_loss:0.31413, valid_loss:0.47001\n",
      "評価データでの正解率 : 0.8547904191616766\n"
     ]
    }
   ],
   "source": [
    "task = Task()\n",
    "for h in [32, 64, 128, 256, 512]:\n",
    "    model = CNNClassifier(len(cnn_vocab_dict), 300, h, 4)\n",
    "    init_cnn_embed(model.embed)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.02, momentum=0.9, nesterov=True)\n",
    "    trainer = Trainer(model, loaders, task, optimizer, 10, device)\n",
    "    trainer.train()\n",
    "    predictor = Predictor(model, gen_loader(cnn_test_dataset, 1), device)\n",
    "    pred = predictor.predict()\n",
    "    print('評価データでの正解率 :', accuracy(test_t, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 89. 事前学習済み言語モデルからの転移学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "huggingface/transformers( https://github.com/huggingface/transformers )を使う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertConfig, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_for_bert(filename):\n",
    "    with open(filename) as f:\n",
    "        dataset = f.read().splitlines()\n",
    "    dataset = [line.split('\\t') for line in dataset]\n",
    "    dataset_t = [categories.index(line[0]) for line in dataset]\n",
    "    dataset_x = [torch.tensor(tokenizer.encode(line[1]), dtype=torch.long) for line in dataset]\n",
    "    return dataset_x, torch.tensor(dataset_t, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_train_x, bert_train_t = read_for_bert('data/train.txt')\n",
    "bert_valid_x, bert_valid_t = read_for_bert('data/valid.txt')\n",
    "bert_test_x, bert_test_t = read_for_bert('data/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_train_dataset = CNNDataset(bert_train_x, bert_train_t)\n",
    "bert_valid_dataset = CNNDataset(bert_valid_x, bert_valid_t)\n",
    "bert_test_dataset = CNNDataset(bert_test_x, bert_test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        config = BertConfig.from_pretrained('bert-base-cased', num_labels=4)\n",
    "        self.bert = BertForSequenceClassification.from_pretrained('bert-base-cased', config=config)\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        x = self.bert(batch.source, attention_mask=batch.mask)\n",
    "        return x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train_loss:0.56885, valid_loss:0.28813\n",
      "epoch 1, train_loss:0.21939, valid_loss:0.26695\n",
      "epoch 2, train_loss:0.13502, valid_loss:0.25614\n",
      "epoch 3, train_loss:0.08685, valid_loss:0.28919\n",
      "epoch 4, train_loss:0.05844, valid_loss:0.30436\n"
     ]
    }
   ],
   "source": [
    "model = BertClassifier()\n",
    "loaders = (\n",
    "    gen_maxtokens_loader(bert_train_dataset, 1000),\n",
    "    gen_descending_loader(bert_valid_dataset, 32))\n",
    "task = Task()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "trainer = Trainer(model, loaders, task, optimizer, 5, device)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習データでの正解率 : 0.9911081991763384\n"
     ]
    }
   ],
   "source": [
    "predictor = Predictor(model, gen_loader(bert_train_dataset, 1), device)\n",
    "pred = predictor.predict()\n",
    "print('学習データでの正解率 :', accuracy(train_t, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "評価データでの正解率 : 0.9273952095808383\n"
     ]
    }
   ],
   "source": [
    "predictor = Predictor(model, gen_loader(bert_test_dataset, 1), device)\n",
    "pred = predictor.predict()\n",
    "print('評価データでの正解率 :', accuracy(test_t, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
